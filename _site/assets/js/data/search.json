[
  
  {
    "title": "在性能倒退发生之前修复它们",
    "url": "/posts/fixing-performance-regressions/",
    "categories": "Code",
    "tags": "性能测试",
    "date": "2022-04-06 13:49:37 +0800",
    





    
    "snippet": "Netflix 有2.22亿用户，在 1700 多种设备上运行，从最先进的智能电视到低成本的移动设备。在 Netflix ，我们为我们的可靠性感到自豪，我们希望保持这种状态。为此，重要的是我们要防止显著的性能倒退到达生产应用程序。缓慢的滚动或延迟的渲染是令人沮丧的，并引发意外的导航。断断续续的播放会降低观看节目的乐趣。任何进入产品发行版的性能倒退都会降低用户体验，因此挑战在于在发布之前检测和...",
    "content": "Netflix 有2.22亿用户，在 1700 多种设备上运行，从最先进的智能电视到低成本的移动设备。在 Netflix ，我们为我们的可靠性感到自豪，我们希望保持这种状态。为此，重要的是我们要防止显著的性能倒退到达生产应用程序。缓慢的滚动或延迟的渲染是令人沮丧的，并引发意外的导航。断断续续的播放会降低观看节目的乐趣。任何进入产品发行版的性能倒退都会降低用户体验，因此挑战在于在发布之前检测和修复这些倒退。这篇文章描述了 Netflix 的 TVUI 团队如何实现一个强大的策略，在性能异常发布之前快速、轻松地检测到它们————通常是在它们提交到代码库之前。我们所说的性能是什么意思？从技术上来说，“性能”指标是指那些与应用的响应性或延迟相关的指标，包括启动时间。但电视设备也往往比其他设备更受内存限制，因此在内存峰值时更容易崩溃——所以对于 Netflix 电视，我们实际上关心的内存至少和性能一样多，甚至可能更多。在 Netflix ，“性能”这个术语通常包括性能指标(严格意义上)和内存指标，这就是我们在这里使用这个术语的方式。为什么要在提交时运行性能测试?由于我们无法收集尚未发布的代码的实时度量，因此很难推断预生产代码的性能概要。我们确实在发布前删减了一个金丝雀版本，这是 Netflix 员工的错误做法，并遵循与生产版本相同的指标收集。虽然 canary 发行版对于即将发布的版本是一个有用的试运行，但它有时会错过回归，因为 canary 用户基础只是生产版本的一小部分。而且，如果在金丝雀中检测到倒退，它仍然需要一个通常混乱和耗时的恢复或补丁。通过对每次提交(合并前和合并后)运行性能测试，我们可以更早地检测潜在的递减提交。我们越早检测到此类提交，后续构建受到的影响就越少，也越容易恢复。理想情况下，我们可以在回归到达主要分支之前捕获它们。什么是性能测试？我们的 TVUI 性能测试的目标是收集内存和响应性指标，同时模拟用户与 Netflix TV 的全方位交互。大约有 50 项绩效测试，每一项都旨在重现用户参与的一个方面。 目标是使每个测试保持简短并专注于特定的、独立的功能（启动、配置文件切换、滚动标题、选择剧集、播放等），而整个测试套件应涵盖整个用户体验以最少的重复。 通过这种方式，我们可以并行运行多个测试，并且没有长杆测试可以使整个测试时间保持可控，并允许重复测试运行。 每个测试都在设备（物理和虚拟）和平台版本 ( SDK ) 的组合上运行。 我们将每个独特的测试/设备/SDK 组合称为测试变体。我们在每个 Pull Request (PR) 中运行两次完整的性能套件:  首次提交 PR 时  当 PR 合并到目标分支时测量每个性能测试都跟踪记忆或响应能力。 这两个指标都会在测试过程中波动，因此我们会在整个测试过程中定期发布指标值。 为了比较测试运行，我们需要一种方法来将该范围内的观察值合并为一个值。我们做出了以下决定：内存测试：使用在测试运行期间观察到的最大内存值(因为这个值决定了设备是否会崩溃)。响应性测试：使用在测试运行期间观察到的中值（基于感知到的缓慢受所有响应影响的假设，而不仅仅是最糟糕的响应）。有哪些挑战？当 Netflix 在生产环境中运行时，我们会捕获实时性能数据，这使得对应用程序性能做出断言变得相对容易。 评估预生产代码的性能（合并到主分支但尚未发布的更改）的性能要困难得多，而且在 PR 中获得未合并代码的性能信号更加困难。 性能测试指标不如实时使用指标有几个原因：  数据量：在 Netflix 应用程序中，相同的步骤重复了数十亿次，但开发人员的速度和资源限制决定了性能测试每次构建只能运行几次。  模拟：无论我们的测试过程多么严格或多么有创意，我们只能近似真实用户的体验，而不能复制它。 真实用户经常一次使用 Netflix 几个小时，每个用户都有不同的偏好和习惯。  噪声：理想情况下，运行任何给定测试变体的给定代码库将始终返回相同的结果。 实际上，这永远不会发生：没有两个设备 CPU 是相同的，垃圾收集不是完全可预测的，API 请求量和后端活动是可变的——功率水平和网络带宽也是如此。 对于每个测试，都会有背景噪音，我们需要以某种方式从我们的分析中滤除。初步方法：静态阈值对于我们的第一次性能验证尝试，我们为内存指标指定了可接受的最大阈值。这种方法背后有一个合理的理由——当电视运行 Netflix 时，内存占用有一个硬限制，超过这个限制 Netflix 就有可能崩溃。静态阈值方法存在几个问题：  每个测试的定制准备工作： 由于每个测试变体都有一个独特的内存配置文件，因此必须根据具体情况研究和分配适当的静态阈值。 这既困难又耗时，因此我们只为大约 30% 的测试变体分配了阈值。  缺乏背景： 作为一种验证技术，静态阈值被证明有些武断。 想象一下，一次提交将内存使用量增加了 10%，但达到了刚好低于阈值的水平。 下一次提交可能是 README 更改（零内存影响），但由于设备背景噪音的正常变化，该指标可能会增加，足以超过阈值。  背景方差没有被过滤： 一旦代码库碰到内存阈值，背景设备噪声就成为决定测试结果落在阈值线哪一侧的主要因素。使用静态阈值技术的不可靠回归信号  警报后调整： 我们发现自己反复提高阈值以使其远离背景噪音要点：异常和变点检测很明显，我们需要一种性能验证技术：  失败偏差无论结果如何，通过对所有测试运行给予同等权重来消除。  不孤立地处理性能数据点，而是评估构建相对于先前构建的性能影响。  自动应用于每个测试无需预先研究、数据输入或持续的人工干预即可  可以同样适用于任何类型的测试数据：内存、响应能力或任何其他非布尔测试数据  最小化背景噪声的影响通过将方差优先于绝对值来  提高洞察力通过在创建时和追溯时检查数据点来我们确定了一个两管齐下的方法：  异常检测通过与最近的过去数据进行比较，立即调用潜在的性能回归  变点检测通过检查过去和未来的数据集群来识别更微妙的性能变化异常检测我们将异常定义为比最近平均值高出 n 个 标准差的任何度量数据点，其中最近的平均值和标准差是从之前的 m 个测试运行中得出的。 对于 Netflix TV 性能测试，我们目前将 n 设置为 4， 将m 设置为 40，但可以调整这些值以最大化信噪比。 当检测到异常时，测试状态设置为 失败 并生成警报。异常检测之所以有效，是因为阈值是动态的并且源自现有数据。 如果数据表现出大量背景方差，则异常阈值将增加以考虑额外的噪声。变点变点是两个不同数据分布模式边界的数据点。 我们使用一种称为 e-divisive 的技术来分析最近的 100 次测试运行，并使用基于 此实现的 Python 实现 。由于我们只对性能回归感兴趣，因此我们忽略了趋于下降的变点。 当为测试检测到变点时，我们不会使测试失败或生成警报（我们认为变点是对异常模式的警告，而不是完整的错误断言）。如您所见，变点是一个更微妙的信号。 它们不一定表示回归，但它们建议对后续数据分布产生影响的构建。在多个测试中生成变点的构建需要进一步调查，然后才能包含在候选版本中。变点让我们对回归检测更有信心，因为它们忽略了误报，例如一次性数据峰值。 因为变点检测需要事后数据，所以它们最适合识别已经在主分支中但尚未发布的潜在回归代码。额外的调整测试运行次数为了解决失败偏差，我们决定将所有测试运行 3 次，无论结果如何。 我们选择了 3 次迭代来提供足够的数据来消除大多数设备噪音（测试随机分配给设备），而不会造成生产力瓶颈。跨测试运行总结接下来，我们需要确定一种方法，将每批 3 次运行的结果压缩为一个值。 目标是忽略由不稳定的设备行为引起的异常结果。最初我们取了这三个运行的平均值，但这导致了过多的误报，因为最不规则的测试运行对结果产生了太大的影响。 切换到中值消除了其中一些误报，但我们仍然收到数量不可接受的过多警报（因为在设备噪音高的时期，我们偶尔会看到三分之二的异常结果）。 最后，由于我们注意到异常值结果往往高于正常值——很少低于正常值——我们决定在 3 次运行中使用最小值，这被证明是消除外部噪声最有效的方法。所有数据点（每次构建 3 次运行)选择每个构建的中值选择每个构建的最小值结果是什么？在将我们的性能验证切换为使用异常和变更点检测后，我们注意到了一些改进。a) 我们很少收到关于潜在性能退化的警报，而当我们确实收到警报时，它更有可能表明真正的回归。 由于不再需要在每次误报后手动增加静态性能阈值，我们的工作量进一步减少。下表代表去年两个不同月份的警报摘要。 2021 年 3 月，我们仍然使用静态阈值进行回归警报。 到 2021 年 10 月，我们已将异常检测用于回归警报。 真正回归的警报 是可疑回归结果既显着又持久的警报提交的数量。请注意，由于 3 月份的测试仅在手动设置阈值时进行验证，因此 10 月份的验证测试运行总数要多得多，但我们仍然只收到 10% 的警报。b) 我们不会对继承先前构建的回归提交的后续无害构建发出警报。 （使用静态阈值技术，所有后续构建都会收到警报，直到回归构建被恢复。）这是因为回归构建会增加均值和标准偏差，因此会将后续非回归构建轻松地置于警报阈值以下。回归构建高于警报阈值后续构建很容易低于警报阈值c) 针对 PR 的性能测试，几乎一直是红色的（因为至少有一个静态阈值被突破的可能性一直很高），现在大部分是绿色的。 当性能测试 为 红色时，我们对真正的性能回归有更高的信心。d) 显示每个构建的异常和变更点计数提供了一个可视化快照，可以快速突出潜在有问题的构建。下一步是什么？进一步的工作我们还有几件事需要改进  更容易确定回归是否是由外部代理引起的： 通常检测到的回归虽然是真实的，但不是提交代码的结果，而是由于外部因素，例如升级到我们的平台依赖项之一，或者打开的功能标志。 在我们的警报摘要中总结外部变化会很有帮助。  在确定验证基线时排除已解决的回归：在生成最近的均值和标准差值时，我们可以通过过滤掉以前已修复的回归中的数据来改进回归检测。  提高开发人员速度： 我们可以通过删除测试中不必要的迭代、添加更多设备以确保可用性以及不再强调对性能不太重要的应用程序部分的测试来进一步减少总测试时间。 我们还可以预先构建应用程序包（至少部分地），这样测试套件就不会因等待新构建而延迟。  更密切地反映生产应用程序收集的指标： 在部署的 Netflix TV 应用程序中，我们收集额外的指标，例如 TTR（渲染时间）和空盒率（视口中的标题丢失图像的频率）。 虽然测试指标和在实际使用期间收集的指标不能直接比较，但测量预生产构建中指标的相对变化可以帮助我们预测生产中的回归。更广泛的采用和新的用例此时，异常和变更点检测将应用于 TVUI 存储库中的每个提交，并且正在部署到提交到 TV Player 存储库（管理播放操作的层）的过程中。 其他 Netflix 团队（电视平台之外）也表达了对这些技术的兴趣，最终目标是标准化整个 Netflix 的回归检测。异常和变更点检测完全独立于框架——唯一需要的输入是当前值和一组要与之比较的最近值。 因此，它们的实用性远远超出了性能测试。 例如，我们正在考虑使用这些技术来监控非基于性能的测试套件的可靠性——在这种情况下，感兴趣的指标是运行到完成的测试的百分比。将来，我们计划将异常和变更点逻辑与我们的测试基础设施分离，并将其作为独立的开源库提供。总结通过使用技术来评估与相邻构建的性能特征（幅度、方差、趋势）相关的构建的性能影响，我们可以更自信地将真正的回归与因其他原因（例如继承的代码、以前的构建或由于测试异常而导致的一次性数据峰值）。 我们还花费更少的时间来追踪误报，并且不再需要为每个结果手动分配阈值——数据本身现在动态设置阈值。这种提高的效率和更高的置信度有助于我们在回归到我们的成员之前快速识别和修复回归。此处讨论的异常和变化点技术可用于识别任何按时间顺序排列的定量数据中的回归（或进展）、意外值或拐点。 它们的效用远远超出了性能分析。 例如，它们可用于识别系统可靠性、客户满意度、产品使用、下载量或收入方面的拐点。我们鼓励您在自己的数据上尝试这些技术。 我们很想更多地了解他们在其他情况下的成功（或其他方式）！  本文译自：Fixing Performance Regressions Before they Happen"
  },
  
  {
    "title": "Java命名规范",
    "url": "/posts/java-name/",
    "categories": "Code",
    "tags": "Java",
    "date": "2021-01-02 10:38:01 +0800",
    





    
    "snippet": "  【强制】所有编程相关的命名均不能以下划线或美元符号开始，也不能以下划线或美元符号结束。反例：MAX_COUNT / EXPIRED_TIME  【强制】抽象类命名使用 Abstract 或 Base 开头；异常类命名使用 Exception 结尾，测试类命名以它要测试的类的名称开始，以 Test 结尾。  【强制】类型与中括号紧挨相连来定义数组。正例：定义整形数组 int[] array...",
    "content": "  【强制】所有编程相关的命名均不能以下划线或美元符号开始，也不能以下划线或美元符号结束。反例：MAX_COUNT / EXPIRED_TIME  【强制】抽象类命名使用 Abstract 或 Base 开头；异常类命名使用 Exception 结尾，测试类命名以它要测试的类的名称开始，以 Test 结尾。  【强制】类型与中括号紧挨相连来定义数组。正例：定义整形数组 int[] arrayDemo。反例：在 main 参数中，使用 String args[] 来定义。  【强制】POJO 类中的任何布尔类型的变量，都不要加 is 前缀，否则部分框架解析会引起序列化错误。说明：本文 MySQL 规约中的建表约定第 1 条，表达是与否的变量采用 is_xxx 的命名方式，所以需要在  设置从 is_xxx 到 xxx 的映射关系。反例：定义为基本数据类型 Boolean isDeleted 的属性，它的方法也是 isDeleted()，框架在反向解析时，“误以为”对应的属性名称是 deleted，导致属性获取不到，进而抛出异常。  【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。正例：应用工具类包名为 com.alibaba.ei.kunlun.aap.util；类名为 MessageUtils（此规则参考 spring 的框架结构）。  【强制】避免在子父类的成员变量之间、或者不同代码块的局部变量之间采用完全相同的命名，使可理解性降低。说明：子类、父类成员变量名相同，即使是 public 也是能够通过编译，而局部变量在同一方法内的不同代码块中同名也是合法的，但是要避免使用。对于非 setter / getter 的参数名称也要避免与成员变量名称相同。反例：public class ConfusingName {    protected int stock;    protected String alibaba;    // 非 setter/getter 的参数名称，不允许与本类成员变量同名    public void access(String alibaba) {        if (condition) {            final int money = 666;            // ...        }        for (int i = 0; i &lt; 10; i++) {            // 在同一方法体中，不允许与其它代码块中的 money 命名相同            final int money = 15978;            // ...        }    }}class Son extends ConfusingName {    // 不允许与父类的成员变量名称相同    private int stock;}  【强制】杜绝完全不规范的英文缩写，避免望文不知义。反例：AbstractClass“缩写”成 AbsClass；condition“缩写”成 condi；Function“缩写”成Fu，此类随意缩写严重降低了代码的可阅读性。  【推荐】为了达到代码自解释的目标，任何自定义编程元素在命名时，使用完整的单词组合来表达。正例：在 JDK 中，对某个对象引用的 volatile 字段进行原子更新的类名为AtomicReferenceFieldUpdater。反例：常见的方法内变量为 int a; 的定义方式。  【推荐】在常量与变量命名时，表示类型的名词放在词尾，以提升辨识度。正例：startTime / workQueue / nameList / TERMINATED_THREAD_COUNT反例：startedAt / QueueOfWork / listName / COUNT_TERMINATED_THREAD  【推荐】如果模块、接口、类、方法使用了设计模式，在命名时要体现出具体模式。说明：将设计模式体现在名字中，有利于阅读者快速理解架构设计思想。正例：public classOrderFactory;正例：public class LoginProxy;正例：public class ResourceObserver;  【推荐】接口类中的方法和属性不要加任何修饰符号（public 也不要加），保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义常量，如果一定要定义，最好确定该常量与接口的方法相关，并且是整个应用的基础常量。正例：接口方法签名 void commit();正例：接口基础常量 String COMPANY = “alibaba”;反例：接口方法定义 public abstract void commit();说明：JDK8 中接口允许有默认实现，那么这个 default 方法，是对所有实现类都有价值的默认实现。  接口和实现类的命名有两套规则：          【强制】对于 Service 和 DAO 类，基于 SOA 的理念，暴露出来的服务一定是接口，内部的实现类用 Impl 的后缀与接口区别。  正例：CacheServiceImpl 实现 CacheService 接口。      【推荐】如果是形容能力的接口名称，取对应的形容词为接口名（通常是 –able 结尾的形容词）。  正例：AbstractTranslator 实现 Translatable。        【参考】枚举类名带上 Enum 后缀，枚举成员名称需要全大写，单词间用下划线隔开。说明：枚举其实就是特殊的常量类，且构造方法被默认强制是私有。正例：枚举名字为 ProcessStatusEnum 的成员名称：SUCCESS / UNKNOWN_REASON  【参考】各层命名规约：          Service / DAO 层方法命名规约：              获取单个对象的方法用 get 做前缀。      获取多个对象的方法用 list 做前缀，复数结尾，如：listObjects      获取统计值的方法用 count 做前缀。      插入的方法用 save / insert 做前缀。      删除的方法用 remove / delete 做前缀。      修改的方法用 update 做前缀。     - 领域模型命名规约：      数据对象：xxxDO，xxx 即为数据表名。      数据传输对象：xxxDTO，xxx 为业务领域相关的名称。      展示对象：xxxVO，xxx 一般为网页名称。      POJO 是 DO / DTO / BO / VO 的统称，禁止命名成 xxxPOJO。      "
  }
  
]

